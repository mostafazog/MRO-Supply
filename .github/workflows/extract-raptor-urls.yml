name: Extract Raptor Individual Product URLs

on:
  workflow_dispatch:
    inputs:
      total_collections:
        description: 'Total number of collection pages to process'
        required: true
        default: '100'
      workers:
        description: 'Number of parallel workers'
        required: true
        default: '10'
      use_browser:
        description: 'Use browser automation'
        required: false
        default: 'true'

jobs:
  extract:
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 10
      matrix:
        worker_id: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium undetected-chromedriver

      - name: Install Chrome
        if: github.event.inputs.use_browser == 'true'
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser chromium-chromedriver

      - name: Run URL extractor
        env:
          WORKER_ID: ${{ matrix.worker_id }}
          TOTAL_WORKERS: ${{ github.event.inputs.workers }}
          TOTAL_COLLECTIONS: ${{ github.event.inputs.total_collections }}
          USE_BROWSER: ${{ github.event.inputs.use_browser }}
        run: |
          python raptorsupplies_url_extractor_worker.py

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: worker-${{ matrix.worker_id }}-urls
          path: |
            worker_${{ matrix.worker_id }}_individual_urls.json
            worker_${{ matrix.worker_id }}_extraction_errors.json
          retention-days: 7

  aggregate:
    needs: extract
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Aggregate URLs
        run: |
          python raptorsupplies_url_aggregator.py artifacts/

      - name: Upload aggregated results
        uses: actions/upload-artifact@v4
        with:
          name: individual-product-urls
          path: |
            raptor_individual_products.json
            raptor_individual_products_urls.txt
            url_extraction_summary.json
          retention-days: 30

      - name: Create summary
        run: |
          python << 'EOF'
          import json
          import os

          if os.path.exists('url_extraction_summary.json'):
              with open('url_extraction_summary.json', 'r') as f:
                  summary = json.load(f)

              print(f"## ðŸŽ¯ URL Extraction Complete")
              print(f"")
              print(f"- **Collections Processed:** {summary.get('collections_processed', 0):,}")
              print(f"- **Workers:** {summary.get('workers', 0)}")
              print(f"- **Individual URLs Found:** {summary.get('individual_urls_found', 0):,}")
              print(f"")
              print(f"### Next Steps")
              print(f"")
              print(f"Download the URLs and run the product scraper:")
              print(f"```bash")
              print(f"gh run download {os.getenv('GITHUB_RUN_ID', 'RUN_ID')}")
              print(f"```")
          else:
              print("âš ï¸ No summary file found")
          EOF
